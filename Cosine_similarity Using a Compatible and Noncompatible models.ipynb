{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27de1449",
   "metadata": {},
   "source": [
    "## <center> COSINE SIMILARITY AND MODEL COMPATIBILITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140878d",
   "metadata": {},
   "source": [
    "#### Cosine similarity between sentences require models to be compatible. The model 'all-MiniLM-L6-v2\" uses sentence transformers and converts sentences into meaningful representations. So, it performs better than the other model \"XLM-RoBERTa\" that was trained on word embeddings. We average pooled the word embeddings into sentence embeddings to compare their performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14683681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deepak\\miniconda3\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tranaformer library is used \n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02c4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two lists of sentences\n",
    "\n",
    "sentences1 = ['He is playing a guitar',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome']\n",
    "\n",
    "sentences2 = ['She is playing violin',\n",
    "              'He is playing violin',\n",
    "              'Dog is playing in ground']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452dccd",
   "metadata": {},
   "source": [
    "### Let us compare results from two models. \n",
    "### First model is \"sentence-transformers/all-MiniLM-L6-v2\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0158e332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0244,  0.0523, -0.0265,  ...,  0.0012,  0.0812, -0.0107],\n",
      "        [ 0.0227, -0.0014, -0.0056,  ..., -0.0225,  0.0846, -0.0283],\n",
      "        [-0.1004, -0.0774, -0.0014,  ..., -0.0010,  0.0718,  0.0221]]) tensor([[-0.0283, -0.0727,  0.0324,  ...,  0.0361,  0.0749,  0.0033],\n",
      "        [-0.0116, -0.0188, -0.0088,  ...,  0.0343,  0.0770,  0.0068],\n",
      "        [ 0.0363, -0.0752,  0.0558,  ...,  0.0059,  0.0366,  0.0845]])\n"
     ]
    }
   ],
   "source": [
    "model1 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "#Compute sentence embedding for both lists. Each sentence will be transformed to one vector. \n",
    "\n",
    "embeddings1 = model1.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model1.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "# Print sentence embeddings to crosscheck\n",
    "print(embeddings1,embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a392533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3509,  0.5447,  0.2525],\n",
       "        [ 0.2461,  0.4481,  0.2023],\n",
       "        [-0.0207, -0.0191,  0.0609]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compute cosine-similarities in nmatrix format\n",
    "\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be20d710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is playing a guitar \t\t\t She is playing violin \t\t\t Score: 0.3509\n",
      "He is playing a guitar \t\t\t He is playing violin \t\t\t Score: 0.5447\n",
      "He is playing a guitar \t\t\t Dog is playing in ground \t\t\t Score: 0.2525\n",
      "A man is playing guitar \t\t\t She is playing violin \t\t\t Score: 0.2461\n",
      "A man is playing guitar \t\t\t He is playing violin \t\t\t Score: 0.4481\n",
      "A man is playing guitar \t\t\t Dog is playing in ground \t\t\t Score: 0.2023\n",
      "The new movie is awesome \t\t\t She is playing violin \t\t\t Score: -0.0207\n",
      "The new movie is awesome \t\t\t He is playing violin \t\t\t Score: -0.0191\n",
      "The new movie is awesome \t\t\t Dog is playing in ground \t\t\t Score: 0.0609\n"
     ]
    }
   ],
   "source": [
    "#Output the pairs with their scores \n",
    "\n",
    "for i in range(len(sentences1)):\n",
    "    for j in range(len(sentences2)):\n",
    "        print(\"{} \\t\\t\\t {} \\t\\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[j], cosine_scores[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c8f55",
   "metadata": {},
   "source": [
    "### A different model \"XLM-RoBERTa-Base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e00daea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\Deepak/.cache\\torch\\sentence_transformers\\xlm-roberta-base. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\Deepak/.cache\\torch\\sentence_transformers\\xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0048,  0.0164, -0.0264,  ...,  0.0148,  0.0108,  0.0944],\n",
      "        [-0.0092,  0.0535, -0.0123,  ...,  0.0061,  0.0381,  0.0835],\n",
      "        [-0.0108,  0.0304, -0.0053,  ..., -0.0031,  0.0459,  0.0180]]) tensor([[-0.0109,  0.0582,  0.0131,  ...,  0.0437,  0.0304,  0.0580],\n",
      "        [-0.0004,  0.0464,  0.0066,  ...,  0.0499,  0.0255,  0.0405],\n",
      "        [-0.0129,  0.0644,  0.0048,  ..., -0.0247,  0.0182, -0.0039]])\n"
     ]
    }
   ],
   "source": [
    "model2 = SentenceTransformer('xlm-roberta-base')\n",
    "\n",
    "#Compute sentence embedding for both lists. Each sentence will be transformed to one vector. \n",
    "\n",
    "embeddings1 = model2.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model2.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "# Print sentence embeddings to crosscheck\n",
    "print(embeddings1,embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ffda91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is playing a guitar \t\t\t She is playing violin \t\t\t Score: 0.9984\n",
      "He is playing a guitar \t\t\t He is playing violin \t\t\t Score: 0.9987\n",
      "He is playing a guitar \t\t\t Dog is playing in ground \t\t\t Score: 0.9971\n",
      "A man is playing guitar \t\t\t She is playing violin \t\t\t Score: 0.9985\n",
      "A man is playing guitar \t\t\t He is playing violin \t\t\t Score: 0.9987\n",
      "A man is playing guitar \t\t\t Dog is playing in ground \t\t\t Score: 0.9976\n",
      "The new movie is awesome \t\t\t She is playing violin \t\t\t Score: 0.9977\n",
      "The new movie is awesome \t\t\t He is playing violin \t\t\t Score: 0.9978\n",
      "The new movie is awesome \t\t\t Dog is playing in ground \t\t\t Score: 0.9967\n"
     ]
    }
   ],
   "source": [
    "#Output the pairs with their scores \n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "for i in range(len(sentences1)):\n",
    "    for j in range(len(sentences2)):\n",
    "        print(\"{} \\t\\t\\t {} \\t\\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[j], cosine_scores[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6575ff",
   "metadata": {},
   "source": [
    "### Conclusion: XLM-RoBERTa does not have a sentence transformer model (cell #6). Hence, we averaged all the tokens. Due to this, it is poorly performing. So, model compatibility with task under consideration is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bacece2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "045be617",
   "metadata": {},
   "source": [
    "### Let us compare some common tasks between BERT and T5. Overall, BERT model is known for tasks related to natural language understanding. T5 is more suitable for generation of text. Here, some common high-level APIs were taken between both the models to showcase their differences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad4e4180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deepak\\miniconda3\\envs\\NNExperiments\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Deepak\\miniconda3\\envs\\NNExperiments\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Deepak\\miniconda3\\envs\\NNExperiments\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Result: 0\n",
      "T5 Result: -1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "# BERT for Text Classification:\n",
    "\n",
    "def bert_text_classification(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    predicted_class = outputs.logits.argmax().item()\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# T5 for Text Classification:\n",
    "\n",
    "def t5_text_classification(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "\n",
    "    prompt = \"Classify the sentiment of the following text: '{}'\".format(text)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"Positive\" in generated_text:\n",
    "        return 1\n",
    "    elif \"Negative\" in generated_text:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "# Example usage\n",
    "text = \"This movie is great!\"\n",
    "bert_result = bert_text_classification(text)\n",
    "t5_result = t5_text_classification(text)\n",
    "\n",
    "print(\"BERT Result:\", bert_result)\n",
    "print(\"T5 Result:\", t5_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf95a01a",
   "metadata": {},
   "source": [
    "In the case of BERT, the result 0 indicates a negative sentiment. BERT is a pre-trained model that has been fine-tuned on various tasks, including sentiment analysis. It predicts the sentiment of the input text based on the patterns it has learned during training. In this case, BERT predicted a negative sentiment for the given input, which may seem contradictory since the input text \"This movie is great!\" is generally considered positive. However, BERT's prediction might be influenced by the training data it was fine-tuned on, which might contain different patterns or nuances.\n",
    "\n",
    "Regarding T5, the result -1 indicates an unknown sentiment. In the code provided, if the generated text from T5 does not contain the words \"Positive\" or \"Negative\", it returns -1 to indicate that the sentiment could not be determined. T5 is a text-to-text transfer model primarily designed for text generation tasks. Although it can be used for text classification, it may not perform as well as models specifically fine-tuned for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8950f531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Input length of input_ids is 35, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Output: [{'summary_text': 'the movie was a thrilling rollercoaster ride with unexpected twists and turns. the acting was superb and the visuals were stunning. overall, it was an amazing experience. a'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deepak\\miniconda3\\envs\\NNExperiments\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 200, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Output: [{'summary_text': 'the movie was a thrilling rollercoaster ride with unexpected twists and turns . the acting was superb and the visuals were stunning .'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# BERT Model\n",
    "nlp_bert = pipeline(\"summarization\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\")\n",
    "bert_result = nlp_bert(\"The movie was a thrilling rollercoaster ride with unexpected twists and turns. The acting was superb and the visuals were stunning. Overall, it was an amazing experience.\")\n",
    "print(\"BERT Output:\", bert_result)\n",
    "\n",
    "# T5 Model\n",
    "nlp_t5 = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
    "t5_result = nlp_t5(\"The movie was a thrilling rollercoaster ride with unexpected twists and turns. The acting was superb and the visuals were stunning. Overall, it was an amazing experience.\")\n",
    "print(\"T5 Output:\", t5_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e4e71fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for translation_en_to_fr. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Output: [{'translation_text': 'hello, how are you? hello? hello? hello? hello? hello? hello?'}]\n",
      "T5 Output: [{'translation_text': 'Bonjour, comment êtes-vous?'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# BERT Model\n",
    "nlp_bert = pipeline(\"translation_en_to_fr\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\")\n",
    "bert_result = nlp_bert(\"Hello, how are you?\")\n",
    "print(\"BERT Output:\", bert_result)\n",
    "\n",
    "# T5 Model\n",
    "nlp_t5 = pipeline(\"translation_en_to_fr\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
    "t5_result = nlp_t5(\"Hello, how are you?\")\n",
    "print(\"T5 Output:\", t5_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c52c0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "C:\\Users\\Deepak\\miniconda3\\envs\\NNExperiments\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Output: [{'generated_text': 'Once upon a time,, and and and and and and and and and and and and and and'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Output: [{'generated_text': 'Once upon a time,me'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# BERT Model\n",
    "nlp_bert = pipeline(\"text-generation\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\")\n",
    "bert_result = nlp_bert(\"Once upon a time,\")\n",
    "print(\"BERT Output:\", bert_result)\n",
    "\n",
    "# T5 Model\n",
    "nlp_t5 = pipeline(\"text-generation\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
    "t5_result = nlp_t5(\"Once upon a time,\")\n",
    "print(\"T5 Output:\", t5_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a38cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Input length of input_ids is 36, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Output: [{'summary_text': 'the movie was filled with suspense and thrilling moments. the acting was exceptional and the plot kept me on the edge of my seat. overall, it was a great experience..'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 40. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Output: [{'summary_text': 'the movie was filled with suspense and thrilling moments . the acting was exceptional and the plot kept me on the edge of my seat .'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# BERT Model\n",
    "nlp_bert = pipeline(\"summarization\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\")\n",
    "bert_result = nlp_bert(\"The movie was filled with suspense and thrilling moments. The acting was exceptional and the plot kept me on the edge of my seat. Overall, it was a great experience.\")\n",
    "print(\"BERT Output:\", bert_result)\n",
    "\n",
    "# T5 Model\n",
    "nlp_t5 = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
    "t5_result = nlp_t5(\"The movie was filled with suspense and thrilling moments. The acting was exceptional and the plot kept me on the edge of my seat. Overall, it was a great experience.\")\n",
    "print(\"T5 Output:\", t5_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbefc677",
   "metadata": {},
   "source": [
    "### BERT adds one sentence extra to mention about overall sentiment. While both BERT and T5 can be used for text summarization, BERT's language understanding capabilities may enable it to generate more detailed summaries that include additional information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caf197fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n",
      "Input length of input_ids is 34, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Output: [{'summary_text': 'the movie had a gripping storyline and exceptional performances by the cast. it kept me on the edge of my seat throughout. overall, it was a thrilling experience. and'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 Output: [{'summary_text': 'the movie kept me on the edge of my seat throughout . it had a gripping storyline and exceptional performances . overall, it was a thrilling experience .'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# BERT Model\n",
    "nlp_bert = pipeline(\"summarization\", model=\"bert-base-uncased\", tokenizer=\"bert-base-uncased\")\n",
    "bert_result = nlp_bert(\"The movie had a gripping storyline and exceptional performances by the cast. It kept me on the edge of my seat throughout. Overall, it was a thrilling experience.\")\n",
    "print(\"BERT Output:\", bert_result)\n",
    "\n",
    "# T5 Model\n",
    "nlp_t5 = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
    "t5_result = nlp_t5(\"The movie had a gripping storyline and exceptional performances by the cast. It kept me on the edge of my seat throughout. Overall, it was a thrilling experience.\")\n",
    "print(\"T5 Output:\", t5_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7fd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c94138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNExperiments",
   "language": "python",
   "name": "nnexperiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

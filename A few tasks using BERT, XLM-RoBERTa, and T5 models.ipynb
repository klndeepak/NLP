{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "704b00b2",
   "metadata": {},
   "source": [
    "## Here, I show some tasks using BERT, XLM-RoBERTa, and T5 models. I use some high-level API pipelines available with the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f849f8",
   "metadata": {},
   "source": [
    "BERT Documentation: https://huggingface.co/docs/transformers/model_doc/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e9620",
   "metadata": {},
   "source": [
    "The BERT model was pretrained on Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It is a bidirectional transformer pretrained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d3e57",
   "metadata": {},
   "source": [
    " BERT was designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both directions, left and right context in all layers. Hence, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without much architecture modifications.BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). BERT is not optimal for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89d22d9",
   "metadata": {},
   "source": [
    "### BERT is designed for natural language understanding with tasks specific to (1) Text Classification, (2) Named Entity Recognition (NER), (3) Question Answering, and (4) Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d032a214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deepak\\miniconda3\\envs\\NNExperiments\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "# Initializing a BERT bert-base-uncased style configuration\n",
    "configuration = BertConfig()\n",
    "\n",
    "# Initializing a model from the bert-base-uncased style configuration\n",
    "model = BertModel(configuration)\n",
    "\n",
    "# Accessing the model configuration. BERTConfig() is a class as it stores parameters and they can be changed. \n",
    "configuration = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6972e83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.30.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67bc16fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration.hidden_size, configuration.num_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14233813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 30522\n",
      "Maximum Length: 512\n",
      "Special Tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n"
     ]
    }
   ],
   "source": [
    "#BERT Tokenizer is also a Class\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Create an instance of the BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Access the tokenizer details\n",
    "vocab_size = tokenizer.vocab_size\n",
    "max_length = tokenizer.model_max_length\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "\n",
    "# Print the details\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\"Maximum Length:\", max_length)\n",
    "print(\"Special Tokens:\", special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361efe36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "#Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#Load the pre-trained BERT model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "#The `tokenizer` function takes the input text as an argument and returns a dictionary of tokenized inputs. \n",
    "#The `return_tensors=\"pt\"` argument specifies that the tokenizer should return PyTorch tensors.\n",
    "#Tokenize the input text using the tokenizer:\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "\n",
    "#Pass the tokenized inputs through the BERT model:\n",
    "# The two stars `**` in `model(**inputs)` are used for unpacking the dictionary `inputs` and passing its key-value pairs \n",
    "# as keyword arguments to the `model` function. This syntax is known as \"dictionary unpacking\" or \"keyword argument \n",
    "#unpacking\" in Python.\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264d9999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us what inputs does?\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f77edf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]) tensor([[0, 0, 0, 0, 0, 0, 0, 0]]) tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "#inputs returns dictionary of key-value pairs for input_ids, token_type_ids, and attention_mask\n",
    "# Access different keys:\n",
    "print(inputs['input_ids'], inputs['token_type_ids'],inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe4f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each token is given an unique id called input_id. If all tokens belong to same segment then token_type_ids return 0. \n",
    "# attention_mask tells which tokens should be attended first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f33313e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1144,  0.1937,  0.1250,  ..., -0.3827,  0.2107,  0.5407],\n",
       "         [ 0.5308,  0.3207,  0.3665,  ..., -0.0036,  0.7579,  0.0388],\n",
       "         [-0.4877,  0.8849,  0.4256,  ..., -0.6976,  0.4458,  0.1231],\n",
       "         ...,\n",
       "         [-0.7003, -0.1815,  0.3297,  ..., -0.4838,  0.0680,  0.8901],\n",
       "         [-1.0355, -0.2567, -0.0317,  ...,  0.3197,  0.3999,  0.1795],\n",
       "         [ 0.6080,  0.2610, -0.3131,  ...,  0.0311, -0.6283, -0.1994]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-7.1946e-01, -2.1445e-01, -2.9576e-01,  3.6603e-01,  2.7968e-01,\n",
       "          2.2183e-02,  5.7299e-01,  6.2331e-02,  5.9585e-02, -9.9965e-01,\n",
       "          5.0145e-02,  4.4756e-01,  9.7612e-01,  3.3989e-02,  8.4494e-01,\n",
       "         -3.6905e-01,  9.8648e-02, -3.7169e-01,  1.7371e-01,  1.1515e-01,\n",
       "          4.4133e-01,  9.9525e-01,  3.7221e-01,  8.2881e-02,  2.1402e-01,\n",
       "          6.8965e-01, -6.1042e-01,  8.7136e-01,  9.4158e-01,  5.7372e-01,\n",
       "         -3.2187e-01,  8.6672e-03, -9.8611e-01, -2.0542e-02, -4.3756e-01,\n",
       "         -9.8012e-01,  1.1142e-01, -6.7587e-01,  1.3499e-01,  3.1130e-01,\n",
       "         -8.2997e-01,  1.9006e-01,  9.9896e-01, -3.1798e-01,  2.1518e-02,\n",
       "         -1.6531e-01, -9.9943e-01,  1.0173e-01, -8.1811e-01,  3.3120e-02,\n",
       "          3.6740e-01, -7.3228e-02, -1.4261e-01,  1.8907e-01,  2.6119e-01,\n",
       "          4.1582e-01, -2.4427e-01, -5.9846e-02, -7.3492e-02, -3.4202e-01,\n",
       "         -5.8001e-01,  2.8331e-01, -5.0513e-01, -8.1967e-01,  1.9814e-01,\n",
       "          1.9108e-01,  3.7011e-02, -1.1327e-01,  1.3472e-01, -2.1614e-01,\n",
       "          6.3494e-01,  2.4869e-02,  3.8287e-01, -8.1779e-01, -2.4874e-01,\n",
       "          8.4982e-02, -5.2998e-01,  1.0000e+00, -5.2155e-02, -9.7052e-01,\n",
       "          3.9848e-01,  2.1361e-02,  3.9035e-01,  3.5588e-01, -1.7881e-01,\n",
       "         -9.9997e-01,  2.6939e-01, -3.8057e-02, -9.8657e-01,  6.9322e-02,\n",
       "          3.9138e-01, -2.1884e-02, -9.6330e-02,  3.8545e-01, -3.4136e-01,\n",
       "         -8.0363e-02, -3.2023e-02, -3.6328e-01, -7.8130e-02,  1.9191e-02,\n",
       "         -1.3429e-01, -1.6014e-02, -5.2640e-02, -2.8006e-01,  9.3612e-02,\n",
       "         -2.2885e-01, -1.2305e-01, -1.1002e-01, -3.2808e-01,  4.0356e-01,\n",
       "          2.8048e-01, -2.0102e-01,  2.7685e-01, -9.4023e-01,  4.1756e-01,\n",
       "         -1.5473e-01, -9.7553e-01, -4.3003e-01, -9.8546e-01,  5.9158e-01,\n",
       "          3.7343e-02, -1.9320e-01,  9.1691e-01,  3.6011e-01,  1.4505e-01,\n",
       "          1.5398e-01, -1.0659e-02, -1.0000e+00, -3.1573e-01, -3.1038e-01,\n",
       "          1.6523e-01, -8.0330e-02, -9.6650e-01, -9.4546e-01,  3.6145e-01,\n",
       "          9.0138e-01, -7.2696e-02,  9.9774e-01,  3.7289e-02,  9.3599e-01,\n",
       "          2.5317e-01, -2.0185e-01,  2.9534e-02, -2.3162e-01,  3.4632e-01,\n",
       "         -1.0763e-01, -2.6565e-01,  1.0874e-01,  1.2985e-01,  2.1135e-02,\n",
       "         -9.6284e-02, -7.6358e-02, -6.5151e-02, -8.9277e-01, -2.3465e-01,\n",
       "          9.1176e-01,  7.0428e-02, -2.1429e-01,  3.8197e-01,  3.5892e-02,\n",
       "         -1.6971e-01,  7.0654e-01,  2.4045e-01,  1.5014e-01, -1.9478e-02,\n",
       "          2.1369e-01, -1.7977e-01,  3.5112e-01, -6.0260e-01,  4.1683e-01,\n",
       "          1.8090e-01, -3.2497e-02, -3.0138e-01, -9.7103e-01, -1.3917e-01,\n",
       "          3.5130e-01,  9.8326e-01,  5.2702e-01,  4.8812e-02,  1.3992e-02,\n",
       "         -6.7964e-02,  2.9718e-01, -9.4136e-01,  9.7219e-01, -2.4774e-02,\n",
       "          1.5224e-01, -1.8241e-01,  5.5585e-02, -7.7306e-01, -9.8999e-02,\n",
       "          4.7058e-01, -1.7023e-01, -7.7803e-01,  5.2833e-02, -3.7679e-01,\n",
       "         -4.1296e-02, -4.9612e-01,  1.4171e-01, -1.1803e-01, -1.8995e-01,\n",
       "          5.0383e-02,  9.0623e-01,  7.8828e-01,  5.2288e-01, -3.5274e-01,\n",
       "          2.8563e-01, -8.1494e-01, -1.9622e-01, -9.2975e-02,  5.9311e-02,\n",
       "          3.1903e-02,  9.8860e-01, -3.9452e-01,  1.1867e-01, -8.6977e-01,\n",
       "         -9.7789e-01, -1.4859e-01, -7.7064e-01, -4.0621e-03, -4.1152e-01,\n",
       "          3.2578e-01,  1.8777e-01, -2.4501e-01,  2.6668e-01, -7.9329e-01,\n",
       "         -4.8133e-01,  9.3245e-02, -1.7010e-01,  2.7043e-01, -3.5880e-02,\n",
       "          7.7973e-01,  4.6697e-01, -3.4636e-01,  5.5237e-02,  9.0312e-01,\n",
       "         -2.4115e-01, -6.4200e-01,  4.1441e-01, -9.7797e-02,  6.2983e-01,\n",
       "         -4.1787e-01,  9.4069e-01,  4.9285e-01,  3.6058e-01, -8.7901e-01,\n",
       "         -2.6726e-01, -5.4679e-01,  9.3854e-04, -1.0502e-02, -4.6837e-01,\n",
       "          3.1116e-01,  3.6999e-01,  1.3306e-01,  6.4092e-01, -3.5630e-01,\n",
       "          8.8549e-01, -8.9036e-01, -9.3865e-01, -8.1215e-01,  2.7362e-01,\n",
       "         -9.8566e-01,  4.0363e-01,  2.1223e-01, -1.4316e-01, -2.4553e-01,\n",
       "         -2.1144e-01, -9.4728e-01,  5.0806e-01, -9.6621e-02,  8.5571e-01,\n",
       "         -1.0133e-01, -6.7768e-01, -2.8500e-01, -8.9905e-01, -3.3577e-01,\n",
       "          8.9155e-02,  3.2600e-01, -2.6467e-01, -9.2032e-01,  3.4629e-01,\n",
       "          3.3430e-01,  2.1397e-01,  3.0628e-02,  9.3878e-01,  9.9986e-01,\n",
       "          9.6385e-01,  8.3159e-01,  6.2250e-01, -9.8055e-01, -7.3623e-01,\n",
       "          9.9986e-01, -7.8395e-01, -9.9998e-01, -8.7800e-01, -5.0893e-01,\n",
       "          2.3399e-02, -1.0000e+00, -6.1938e-02,  1.9563e-01, -9.0552e-01,\n",
       "         -1.4008e-01,  9.5264e-01,  7.9837e-01, -1.0000e+00,  7.6343e-01,\n",
       "          8.3670e-01, -4.5859e-01,  5.4410e-01, -2.4074e-01,  9.6085e-01,\n",
       "          1.9164e-01,  3.2135e-01, -1.3064e-02,  2.4534e-01, -5.3001e-01,\n",
       "         -5.9538e-01,  3.7464e-01, -2.1190e-01,  8.8024e-01,  1.9648e-02,\n",
       "         -3.8349e-01, -8.4779e-01,  1.4678e-02, -2.8376e-02, -4.4313e-01,\n",
       "         -9.4966e-01, -6.5704e-02, -7.2326e-02,  6.5967e-01, -1.1504e-01,\n",
       "          2.1876e-01, -5.5254e-01,  9.2219e-02, -5.0583e-01, -5.2826e-02,\n",
       "          5.1425e-01, -8.9533e-01, -1.2744e-01,  9.7845e-02, -6.0145e-01,\n",
       "         -3.1653e-02, -9.5186e-01,  9.4685e-01, -2.2341e-01,  1.8390e-01,\n",
       "          1.0000e+00,  1.1756e-01, -7.0390e-01,  3.2502e-01, -1.0898e-02,\n",
       "         -1.8308e-01,  9.9999e-01,  5.8376e-01, -9.7387e-01, -3.3783e-01,\n",
       "          2.9640e-01, -2.7002e-01, -2.2243e-01,  9.9711e-01,  1.4422e-02,\n",
       "          7.8267e-02,  3.8660e-01,  9.7787e-01, -9.8501e-01,  8.7459e-01,\n",
       "         -7.2276e-01, -9.5249e-01,  9.4567e-01,  9.1005e-01, -5.0722e-01,\n",
       "         -4.9026e-01, -1.2517e-01, -3.9076e-02,  8.8128e-02, -8.2481e-01,\n",
       "          3.8301e-01,  1.8045e-01,  5.4796e-02,  8.0041e-01, -3.3501e-01,\n",
       "         -3.9115e-01,  1.4233e-01, -9.0142e-02,  3.4585e-01,  4.4044e-01,\n",
       "          3.1045e-01, -1.3280e-01, -1.3614e-01, -3.0303e-01, -4.8794e-01,\n",
       "         -9.4950e-01,  1.0887e-01,  1.0000e+00,  6.0751e-02,  8.3376e-02,\n",
       "         -3.1304e-03,  8.5578e-02, -3.1288e-01,  2.6283e-01,  2.6870e-01,\n",
       "         -1.4267e-01, -7.4000e-01,  2.2857e-01, -7.9442e-01, -9.8812e-01,\n",
       "          4.3592e-01,  7.7230e-02, -3.8084e-02,  9.9490e-01,  3.2616e-01,\n",
       "          6.7990e-02,  8.2889e-02,  4.7391e-01, -2.1855e-01,  3.9278e-01,\n",
       "          3.7667e-02,  9.6440e-01, -1.8374e-01,  3.9259e-01,  4.3319e-01,\n",
       "         -1.8618e-01, -2.1584e-01, -4.9610e-01, -9.7025e-02, -8.8006e-01,\n",
       "          2.4995e-01, -9.3940e-01,  9.3827e-01,  3.2001e-01,  1.1919e-01,\n",
       "          7.3959e-02,  3.1274e-02,  1.0000e+00, -7.5631e-01,  3.5396e-01,\n",
       "          5.3290e-01,  3.2036e-01, -9.7538e-01, -4.7482e-01, -2.3322e-01,\n",
       "          3.5376e-02, -4.6061e-02, -1.2863e-01,  8.3798e-02, -9.5139e-01,\n",
       "          3.4664e-02,  4.5232e-03, -8.8296e-01, -9.8300e-01,  1.6467e-01,\n",
       "          3.3595e-01, -1.0217e-01, -7.0275e-01, -4.3307e-01, -5.4169e-01,\n",
       "          1.8884e-01, -5.5797e-02, -9.2162e-01,  4.4790e-01, -3.5256e-02,\n",
       "          2.1131e-01, -4.6267e-02,  4.1688e-01,  1.9312e-01,  8.2643e-01,\n",
       "          3.1895e-02,  1.8035e-02,  2.2502e-02, -5.6261e-01,  5.2690e-01,\n",
       "         -4.1523e-01, -2.0335e-01,  5.0974e-03,  1.0000e+00, -1.3769e-01,\n",
       "          4.0090e-01,  4.8581e-01,  3.0547e-01,  1.0161e-01,  1.1372e-01,\n",
       "          5.4688e-01,  1.7282e-01, -1.1611e-01,  1.1691e-01,  3.3706e-01,\n",
       "         -9.4996e-02,  3.3125e-01, -1.1599e-01,  5.5664e-02,  6.9017e-01,\n",
       "          5.2775e-01, -7.8248e-02,  7.7874e-02, -2.5570e-01,  9.5441e-01,\n",
       "          4.4725e-02,  7.5062e-02, -1.6521e-01,  9.8572e-02, -1.2673e-01,\n",
       "          4.2396e-01,  9.9999e-01,  1.4012e-01, -6.5116e-02, -9.8683e-01,\n",
       "         -3.4660e-01, -6.9549e-01,  9.9968e-01,  7.8693e-01, -6.2560e-01,\n",
       "          4.0561e-01,  5.1398e-01, -7.1927e-03,  3.7469e-01, -4.9920e-02,\n",
       "         -1.8379e-01,  1.0699e-01,  6.4272e-02,  9.4363e-01, -4.5982e-01,\n",
       "         -9.6684e-01, -4.8714e-01,  1.6233e-01, -9.2982e-01,  9.8976e-01,\n",
       "         -2.8241e-01, -3.9526e-02, -2.8969e-01,  2.2178e-01, -7.3322e-01,\n",
       "         -1.9752e-01, -9.7385e-01,  1.4625e-01,  1.7385e-02,  9.4459e-01,\n",
       "          8.0070e-02, -4.1026e-01, -7.2363e-01,  6.5496e-02,  2.9531e-01,\n",
       "         -2.0402e-01, -9.4453e-01,  9.4867e-01, -9.6224e-01,  4.1987e-01,\n",
       "          9.9992e-01,  2.0182e-01, -5.9719e-01,  6.7062e-02, -1.3560e-01,\n",
       "          1.1140e-01, -7.1073e-02,  3.3843e-01, -9.1928e-01, -1.1785e-01,\n",
       "          7.1898e-03,  9.3813e-02,  1.2718e-01, -4.2176e-01,  6.2383e-01,\n",
       "         -3.0948e-02, -3.9573e-01, -4.9911e-01,  1.9713e-01,  1.9574e-01,\n",
       "          5.2774e-01, -6.4999e-02,  3.8217e-02, -1.3764e-01,  1.3114e-01,\n",
       "         -8.2896e-01, -6.2802e-02, -1.3078e-01, -9.9745e-01,  3.8189e-01,\n",
       "         -1.0000e+00, -4.9527e-02, -3.3011e-01, -9.7048e-03,  7.4032e-01,\n",
       "          4.5588e-01, -4.3037e-02, -5.9485e-01,  3.5137e-02,  8.4290e-01,\n",
       "          7.0024e-01,  4.9499e-03,  1.5221e-01, -4.8182e-01,  3.4913e-02,\n",
       "          6.8680e-02,  5.9797e-02,  9.4146e-02,  5.7532e-01,  3.5063e-02,\n",
       "          1.0000e+00, -4.4780e-03, -3.4757e-01, -7.9309e-01,  5.7241e-02,\n",
       "         -4.8242e-02,  9.9991e-01, -3.6963e-01, -9.2729e-01,  2.2610e-01,\n",
       "         -3.2602e-01, -6.5948e-01,  2.3506e-01, -6.6026e-02, -6.2875e-01,\n",
       "         -4.7124e-01,  8.3105e-01,  4.3462e-01, -5.2237e-01,  2.1811e-01,\n",
       "         -1.1176e-01, -2.7027e-01, -6.8502e-02,  5.0505e-02,  9.8319e-01,\n",
       "          3.3888e-01,  5.6442e-01,  1.0517e-01,  6.1440e-02,  9.3666e-01,\n",
       "          7.3989e-02, -2.4528e-01, -8.5207e-02,  9.9998e-01,  1.4210e-01,\n",
       "         -8.2488e-01,  2.2405e-01, -9.2098e-01, -1.0235e-01, -8.4105e-01,\n",
       "          2.1140e-01, -3.4106e-02,  8.0942e-01,  4.9839e-03,  8.9624e-01,\n",
       "          6.7184e-02, -1.7137e-01, -2.7561e-01,  2.6385e-01,  1.9073e-01,\n",
       "         -8.6307e-01, -9.8238e-01, -9.8035e-01,  2.2370e-01, -3.5154e-01,\n",
       "          1.9181e-01,  8.9503e-02, -9.8139e-02,  8.3593e-02,  3.0373e-01,\n",
       "         -9.9998e-01,  9.0944e-01,  2.9007e-01,  4.4585e-01,  9.4631e-01,\n",
       "          4.1260e-01,  1.9621e-01,  2.4693e-01, -9.7562e-01, -7.6957e-01,\n",
       "         -1.7996e-01, -5.8601e-02,  4.2949e-01,  3.3341e-01,  8.0548e-01,\n",
       "          2.5306e-01, -4.0736e-01, -3.4586e-02,  4.0999e-01, -8.3874e-01,\n",
       "         -9.9092e-01,  3.0937e-01,  3.3917e-01, -6.2679e-01,  9.4565e-01,\n",
       "         -5.9613e-01, -1.9441e-03,  3.7971e-01, -2.2250e-01,  5.2158e-01,\n",
       "          5.9324e-01, -1.8357e-02, -6.7998e-03,  2.1554e-01,  8.2484e-01,\n",
       "          8.0068e-01,  9.7795e-01, -1.0868e-01,  4.3963e-01,  2.2388e-01,\n",
       "          2.7078e-01,  8.5065e-01, -9.2567e-01,  4.3630e-03, -3.2061e-02,\n",
       "         -1.9565e-01,  1.1169e-01, -9.4711e-02, -7.2644e-01,  6.3986e-01,\n",
       "         -1.7955e-01,  4.2939e-01, -2.0787e-01,  2.2294e-01, -2.3857e-01,\n",
       "          6.7195e-02, -5.1772e-01, -3.6389e-01,  5.3169e-01,  5.3485e-02,\n",
       "          8.5309e-01,  6.4611e-01,  1.2341e-02, -2.4756e-01,  1.4718e-02,\n",
       "         -5.3295e-02, -9.2566e-01,  5.0771e-01,  1.2492e-01,  2.1457e-01,\n",
       "         -6.7958e-02, -2.7113e-01,  9.0946e-01, -1.9032e-01, -2.1274e-01,\n",
       "         -6.4847e-02, -4.3871e-01,  6.3752e-01, -2.1017e-01, -2.9291e-01,\n",
       "         -3.1616e-01,  5.4117e-01,  1.6768e-01,  9.9424e-01, -9.4510e-02,\n",
       "         -2.9022e-01, -2.1886e-03, -1.5720e-01,  2.8317e-01, -2.9364e-01,\n",
       "         -9.9998e-01,  1.4066e-01,  9.1604e-02,  1.1458e-01, -2.1965e-01,\n",
       "          3.0746e-01, -5.7720e-02, -8.7692e-01, -9.3892e-02,  2.2809e-01,\n",
       "          3.8768e-02, -3.2828e-01, -3.1139e-01,  4.1117e-01,  4.6004e-01,\n",
       "          5.5266e-01,  7.2535e-01,  2.5635e-01,  5.2958e-01,  4.7964e-01,\n",
       "         -1.0402e-01, -5.4204e-01,  8.4934e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us see outputs\n",
    "outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d650b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions'>\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs))\n",
    "# Usually it was a tuple. It may be overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ddf3999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1144,  0.1937,  0.1250,  ..., -0.3827,  0.2107,  0.5407],\n",
       "         [ 0.5308,  0.3207,  0.3665,  ..., -0.0036,  0.7579,  0.0388],\n",
       "         [-0.4877,  0.8849,  0.4256,  ..., -0.6976,  0.4458,  0.1231],\n",
       "         ...,\n",
       "         [-0.7003, -0.1815,  0.3297,  ..., -0.4838,  0.0680,  0.8901],\n",
       "         [-1.0355, -0.2567, -0.0317,  ...,  0.3197,  0.3999,  0.1795],\n",
       "         [ 0.6080,  0.2610, -0.3131,  ...,  0.0311, -0.6283, -0.1994]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d799d5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.1144,  0.1937,  0.1250,  ..., -0.3827,  0.2107,  0.5407],\n",
       "          [ 0.5308,  0.3207,  0.3665,  ..., -0.0036,  0.7579,  0.0388],\n",
       "          [-0.4877,  0.8849,  0.4256,  ..., -0.6976,  0.4458,  0.1231],\n",
       "          ...,\n",
       "          [-0.7003, -0.1815,  0.3297,  ..., -0.4838,  0.0680,  0.8901],\n",
       "          [-1.0355, -0.2567, -0.0317,  ...,  0.3197,  0.3999,  0.1795],\n",
       "          [ 0.6080,  0.2610, -0.3131,  ...,  0.0311, -0.6283, -0.1994]]],\n",
       "        grad_fn=<NativeLayerNormBackward0>),\n",
       " tensor([[-7.1946e-01, -2.1445e-01, -2.9576e-01,  3.6603e-01,  2.7968e-01,\n",
       "           2.2183e-02,  5.7299e-01,  6.2331e-02,  5.9585e-02, -9.9965e-01,\n",
       "           5.0145e-02,  4.4756e-01,  9.7612e-01,  3.3989e-02,  8.4494e-01,\n",
       "          -3.6905e-01,  9.8648e-02, -3.7169e-01,  1.7371e-01,  1.1515e-01,\n",
       "           4.4133e-01,  9.9525e-01,  3.7221e-01,  8.2881e-02,  2.1402e-01,\n",
       "           6.8965e-01, -6.1042e-01,  8.7136e-01,  9.4158e-01,  5.7372e-01,\n",
       "          -3.2187e-01,  8.6672e-03, -9.8611e-01, -2.0542e-02, -4.3756e-01,\n",
       "          -9.8012e-01,  1.1142e-01, -6.7587e-01,  1.3499e-01,  3.1130e-01,\n",
       "          -8.2997e-01,  1.9006e-01,  9.9896e-01, -3.1798e-01,  2.1518e-02,\n",
       "          -1.6531e-01, -9.9943e-01,  1.0173e-01, -8.1811e-01,  3.3120e-02,\n",
       "           3.6740e-01, -7.3228e-02, -1.4261e-01,  1.8907e-01,  2.6119e-01,\n",
       "           4.1582e-01, -2.4427e-01, -5.9846e-02, -7.3492e-02, -3.4202e-01,\n",
       "          -5.8001e-01,  2.8331e-01, -5.0513e-01, -8.1967e-01,  1.9814e-01,\n",
       "           1.9108e-01,  3.7011e-02, -1.1327e-01,  1.3472e-01, -2.1614e-01,\n",
       "           6.3494e-01,  2.4869e-02,  3.8287e-01, -8.1779e-01, -2.4874e-01,\n",
       "           8.4982e-02, -5.2998e-01,  1.0000e+00, -5.2155e-02, -9.7052e-01,\n",
       "           3.9848e-01,  2.1361e-02,  3.9035e-01,  3.5588e-01, -1.7881e-01,\n",
       "          -9.9997e-01,  2.6939e-01, -3.8057e-02, -9.8657e-01,  6.9322e-02,\n",
       "           3.9138e-01, -2.1884e-02, -9.6330e-02,  3.8545e-01, -3.4136e-01,\n",
       "          -8.0363e-02, -3.2023e-02, -3.6328e-01, -7.8130e-02,  1.9191e-02,\n",
       "          -1.3429e-01, -1.6014e-02, -5.2640e-02, -2.8006e-01,  9.3612e-02,\n",
       "          -2.2885e-01, -1.2305e-01, -1.1002e-01, -3.2808e-01,  4.0356e-01,\n",
       "           2.8048e-01, -2.0102e-01,  2.7685e-01, -9.4023e-01,  4.1756e-01,\n",
       "          -1.5473e-01, -9.7553e-01, -4.3003e-01, -9.8546e-01,  5.9158e-01,\n",
       "           3.7343e-02, -1.9320e-01,  9.1691e-01,  3.6011e-01,  1.4505e-01,\n",
       "           1.5398e-01, -1.0659e-02, -1.0000e+00, -3.1573e-01, -3.1038e-01,\n",
       "           1.6523e-01, -8.0330e-02, -9.6650e-01, -9.4546e-01,  3.6145e-01,\n",
       "           9.0138e-01, -7.2696e-02,  9.9774e-01,  3.7289e-02,  9.3599e-01,\n",
       "           2.5317e-01, -2.0185e-01,  2.9534e-02, -2.3162e-01,  3.4632e-01,\n",
       "          -1.0763e-01, -2.6565e-01,  1.0874e-01,  1.2985e-01,  2.1135e-02,\n",
       "          -9.6284e-02, -7.6358e-02, -6.5151e-02, -8.9277e-01, -2.3465e-01,\n",
       "           9.1176e-01,  7.0428e-02, -2.1429e-01,  3.8197e-01,  3.5892e-02,\n",
       "          -1.6971e-01,  7.0654e-01,  2.4045e-01,  1.5014e-01, -1.9478e-02,\n",
       "           2.1369e-01, -1.7977e-01,  3.5112e-01, -6.0260e-01,  4.1683e-01,\n",
       "           1.8090e-01, -3.2497e-02, -3.0138e-01, -9.7103e-01, -1.3917e-01,\n",
       "           3.5130e-01,  9.8326e-01,  5.2702e-01,  4.8812e-02,  1.3992e-02,\n",
       "          -6.7964e-02,  2.9718e-01, -9.4136e-01,  9.7219e-01, -2.4774e-02,\n",
       "           1.5224e-01, -1.8241e-01,  5.5585e-02, -7.7306e-01, -9.8999e-02,\n",
       "           4.7058e-01, -1.7023e-01, -7.7803e-01,  5.2833e-02, -3.7679e-01,\n",
       "          -4.1296e-02, -4.9612e-01,  1.4171e-01, -1.1803e-01, -1.8995e-01,\n",
       "           5.0383e-02,  9.0623e-01,  7.8828e-01,  5.2288e-01, -3.5274e-01,\n",
       "           2.8563e-01, -8.1494e-01, -1.9622e-01, -9.2975e-02,  5.9311e-02,\n",
       "           3.1903e-02,  9.8860e-01, -3.9452e-01,  1.1867e-01, -8.6977e-01,\n",
       "          -9.7789e-01, -1.4859e-01, -7.7064e-01, -4.0621e-03, -4.1152e-01,\n",
       "           3.2578e-01,  1.8777e-01, -2.4501e-01,  2.6668e-01, -7.9329e-01,\n",
       "          -4.8133e-01,  9.3245e-02, -1.7010e-01,  2.7043e-01, -3.5880e-02,\n",
       "           7.7973e-01,  4.6697e-01, -3.4636e-01,  5.5237e-02,  9.0312e-01,\n",
       "          -2.4115e-01, -6.4200e-01,  4.1441e-01, -9.7797e-02,  6.2983e-01,\n",
       "          -4.1787e-01,  9.4069e-01,  4.9285e-01,  3.6058e-01, -8.7901e-01,\n",
       "          -2.6726e-01, -5.4679e-01,  9.3854e-04, -1.0502e-02, -4.6837e-01,\n",
       "           3.1116e-01,  3.6999e-01,  1.3306e-01,  6.4092e-01, -3.5630e-01,\n",
       "           8.8549e-01, -8.9036e-01, -9.3865e-01, -8.1215e-01,  2.7362e-01,\n",
       "          -9.8566e-01,  4.0363e-01,  2.1223e-01, -1.4316e-01, -2.4553e-01,\n",
       "          -2.1144e-01, -9.4728e-01,  5.0806e-01, -9.6621e-02,  8.5571e-01,\n",
       "          -1.0133e-01, -6.7768e-01, -2.8500e-01, -8.9905e-01, -3.3577e-01,\n",
       "           8.9155e-02,  3.2600e-01, -2.6467e-01, -9.2032e-01,  3.4629e-01,\n",
       "           3.3430e-01,  2.1397e-01,  3.0628e-02,  9.3878e-01,  9.9986e-01,\n",
       "           9.6385e-01,  8.3159e-01,  6.2250e-01, -9.8055e-01, -7.3623e-01,\n",
       "           9.9986e-01, -7.8395e-01, -9.9998e-01, -8.7800e-01, -5.0893e-01,\n",
       "           2.3399e-02, -1.0000e+00, -6.1938e-02,  1.9563e-01, -9.0552e-01,\n",
       "          -1.4008e-01,  9.5264e-01,  7.9837e-01, -1.0000e+00,  7.6343e-01,\n",
       "           8.3670e-01, -4.5859e-01,  5.4410e-01, -2.4074e-01,  9.6085e-01,\n",
       "           1.9164e-01,  3.2135e-01, -1.3064e-02,  2.4534e-01, -5.3001e-01,\n",
       "          -5.9538e-01,  3.7464e-01, -2.1190e-01,  8.8024e-01,  1.9648e-02,\n",
       "          -3.8349e-01, -8.4779e-01,  1.4678e-02, -2.8376e-02, -4.4313e-01,\n",
       "          -9.4966e-01, -6.5704e-02, -7.2326e-02,  6.5967e-01, -1.1504e-01,\n",
       "           2.1876e-01, -5.5254e-01,  9.2219e-02, -5.0583e-01, -5.2826e-02,\n",
       "           5.1425e-01, -8.9533e-01, -1.2744e-01,  9.7845e-02, -6.0145e-01,\n",
       "          -3.1653e-02, -9.5186e-01,  9.4685e-01, -2.2341e-01,  1.8390e-01,\n",
       "           1.0000e+00,  1.1756e-01, -7.0390e-01,  3.2502e-01, -1.0898e-02,\n",
       "          -1.8308e-01,  9.9999e-01,  5.8376e-01, -9.7387e-01, -3.3783e-01,\n",
       "           2.9640e-01, -2.7002e-01, -2.2243e-01,  9.9711e-01,  1.4422e-02,\n",
       "           7.8267e-02,  3.8660e-01,  9.7787e-01, -9.8501e-01,  8.7459e-01,\n",
       "          -7.2276e-01, -9.5249e-01,  9.4567e-01,  9.1005e-01, -5.0722e-01,\n",
       "          -4.9026e-01, -1.2517e-01, -3.9076e-02,  8.8128e-02, -8.2481e-01,\n",
       "           3.8301e-01,  1.8045e-01,  5.4796e-02,  8.0041e-01, -3.3501e-01,\n",
       "          -3.9115e-01,  1.4233e-01, -9.0142e-02,  3.4585e-01,  4.4044e-01,\n",
       "           3.1045e-01, -1.3280e-01, -1.3614e-01, -3.0303e-01, -4.8794e-01,\n",
       "          -9.4950e-01,  1.0887e-01,  1.0000e+00,  6.0751e-02,  8.3376e-02,\n",
       "          -3.1304e-03,  8.5578e-02, -3.1288e-01,  2.6283e-01,  2.6870e-01,\n",
       "          -1.4267e-01, -7.4000e-01,  2.2857e-01, -7.9442e-01, -9.8812e-01,\n",
       "           4.3592e-01,  7.7230e-02, -3.8084e-02,  9.9490e-01,  3.2616e-01,\n",
       "           6.7990e-02,  8.2889e-02,  4.7391e-01, -2.1855e-01,  3.9278e-01,\n",
       "           3.7667e-02,  9.6440e-01, -1.8374e-01,  3.9259e-01,  4.3319e-01,\n",
       "          -1.8618e-01, -2.1584e-01, -4.9610e-01, -9.7025e-02, -8.8006e-01,\n",
       "           2.4995e-01, -9.3940e-01,  9.3827e-01,  3.2001e-01,  1.1919e-01,\n",
       "           7.3959e-02,  3.1274e-02,  1.0000e+00, -7.5631e-01,  3.5396e-01,\n",
       "           5.3290e-01,  3.2036e-01, -9.7538e-01, -4.7482e-01, -2.3322e-01,\n",
       "           3.5376e-02, -4.6061e-02, -1.2863e-01,  8.3798e-02, -9.5139e-01,\n",
       "           3.4664e-02,  4.5232e-03, -8.8296e-01, -9.8300e-01,  1.6467e-01,\n",
       "           3.3595e-01, -1.0217e-01, -7.0275e-01, -4.3307e-01, -5.4169e-01,\n",
       "           1.8884e-01, -5.5797e-02, -9.2162e-01,  4.4790e-01, -3.5256e-02,\n",
       "           2.1131e-01, -4.6267e-02,  4.1688e-01,  1.9312e-01,  8.2643e-01,\n",
       "           3.1895e-02,  1.8035e-02,  2.2502e-02, -5.6261e-01,  5.2690e-01,\n",
       "          -4.1523e-01, -2.0335e-01,  5.0974e-03,  1.0000e+00, -1.3769e-01,\n",
       "           4.0090e-01,  4.8581e-01,  3.0547e-01,  1.0161e-01,  1.1372e-01,\n",
       "           5.4688e-01,  1.7282e-01, -1.1611e-01,  1.1691e-01,  3.3706e-01,\n",
       "          -9.4996e-02,  3.3125e-01, -1.1599e-01,  5.5664e-02,  6.9017e-01,\n",
       "           5.2775e-01, -7.8248e-02,  7.7874e-02, -2.5570e-01,  9.5441e-01,\n",
       "           4.4725e-02,  7.5062e-02, -1.6521e-01,  9.8572e-02, -1.2673e-01,\n",
       "           4.2396e-01,  9.9999e-01,  1.4012e-01, -6.5116e-02, -9.8683e-01,\n",
       "          -3.4660e-01, -6.9549e-01,  9.9968e-01,  7.8693e-01, -6.2560e-01,\n",
       "           4.0561e-01,  5.1398e-01, -7.1927e-03,  3.7469e-01, -4.9920e-02,\n",
       "          -1.8379e-01,  1.0699e-01,  6.4272e-02,  9.4363e-01, -4.5982e-01,\n",
       "          -9.6684e-01, -4.8714e-01,  1.6233e-01, -9.2982e-01,  9.8976e-01,\n",
       "          -2.8241e-01, -3.9526e-02, -2.8969e-01,  2.2178e-01, -7.3322e-01,\n",
       "          -1.9752e-01, -9.7385e-01,  1.4625e-01,  1.7385e-02,  9.4459e-01,\n",
       "           8.0070e-02, -4.1026e-01, -7.2363e-01,  6.5496e-02,  2.9531e-01,\n",
       "          -2.0402e-01, -9.4453e-01,  9.4867e-01, -9.6224e-01,  4.1987e-01,\n",
       "           9.9992e-01,  2.0182e-01, -5.9719e-01,  6.7062e-02, -1.3560e-01,\n",
       "           1.1140e-01, -7.1073e-02,  3.3843e-01, -9.1928e-01, -1.1785e-01,\n",
       "           7.1898e-03,  9.3813e-02,  1.2718e-01, -4.2176e-01,  6.2383e-01,\n",
       "          -3.0948e-02, -3.9573e-01, -4.9911e-01,  1.9713e-01,  1.9574e-01,\n",
       "           5.2774e-01, -6.4999e-02,  3.8217e-02, -1.3764e-01,  1.3114e-01,\n",
       "          -8.2896e-01, -6.2802e-02, -1.3078e-01, -9.9745e-01,  3.8189e-01,\n",
       "          -1.0000e+00, -4.9527e-02, -3.3011e-01, -9.7048e-03,  7.4032e-01,\n",
       "           4.5588e-01, -4.3037e-02, -5.9485e-01,  3.5137e-02,  8.4290e-01,\n",
       "           7.0024e-01,  4.9499e-03,  1.5221e-01, -4.8182e-01,  3.4913e-02,\n",
       "           6.8680e-02,  5.9797e-02,  9.4146e-02,  5.7532e-01,  3.5063e-02,\n",
       "           1.0000e+00, -4.4780e-03, -3.4757e-01, -7.9309e-01,  5.7241e-02,\n",
       "          -4.8242e-02,  9.9991e-01, -3.6963e-01, -9.2729e-01,  2.2610e-01,\n",
       "          -3.2602e-01, -6.5948e-01,  2.3506e-01, -6.6026e-02, -6.2875e-01,\n",
       "          -4.7124e-01,  8.3105e-01,  4.3462e-01, -5.2237e-01,  2.1811e-01,\n",
       "          -1.1176e-01, -2.7027e-01, -6.8502e-02,  5.0505e-02,  9.8319e-01,\n",
       "           3.3888e-01,  5.6442e-01,  1.0517e-01,  6.1440e-02,  9.3666e-01,\n",
       "           7.3989e-02, -2.4528e-01, -8.5207e-02,  9.9998e-01,  1.4210e-01,\n",
       "          -8.2488e-01,  2.2405e-01, -9.2098e-01, -1.0235e-01, -8.4105e-01,\n",
       "           2.1140e-01, -3.4106e-02,  8.0942e-01,  4.9839e-03,  8.9624e-01,\n",
       "           6.7184e-02, -1.7137e-01, -2.7561e-01,  2.6385e-01,  1.9073e-01,\n",
       "          -8.6307e-01, -9.8238e-01, -9.8035e-01,  2.2370e-01, -3.5154e-01,\n",
       "           1.9181e-01,  8.9503e-02, -9.8139e-02,  8.3593e-02,  3.0373e-01,\n",
       "          -9.9998e-01,  9.0944e-01,  2.9007e-01,  4.4585e-01,  9.4631e-01,\n",
       "           4.1260e-01,  1.9621e-01,  2.4693e-01, -9.7562e-01, -7.6957e-01,\n",
       "          -1.7996e-01, -5.8601e-02,  4.2949e-01,  3.3341e-01,  8.0548e-01,\n",
       "           2.5306e-01, -4.0736e-01, -3.4586e-02,  4.0999e-01, -8.3874e-01,\n",
       "          -9.9092e-01,  3.0937e-01,  3.3917e-01, -6.2679e-01,  9.4565e-01,\n",
       "          -5.9613e-01, -1.9441e-03,  3.7971e-01, -2.2250e-01,  5.2158e-01,\n",
       "           5.9324e-01, -1.8357e-02, -6.7998e-03,  2.1554e-01,  8.2484e-01,\n",
       "           8.0068e-01,  9.7795e-01, -1.0868e-01,  4.3963e-01,  2.2388e-01,\n",
       "           2.7078e-01,  8.5065e-01, -9.2567e-01,  4.3630e-03, -3.2061e-02,\n",
       "          -1.9565e-01,  1.1169e-01, -9.4711e-02, -7.2644e-01,  6.3986e-01,\n",
       "          -1.7955e-01,  4.2939e-01, -2.0787e-01,  2.2294e-01, -2.3857e-01,\n",
       "           6.7195e-02, -5.1772e-01, -3.6389e-01,  5.3169e-01,  5.3485e-02,\n",
       "           8.5309e-01,  6.4611e-01,  1.2341e-02, -2.4756e-01,  1.4718e-02,\n",
       "          -5.3295e-02, -9.2566e-01,  5.0771e-01,  1.2492e-01,  2.1457e-01,\n",
       "          -6.7958e-02, -2.7113e-01,  9.0946e-01, -1.9032e-01, -2.1274e-01,\n",
       "          -6.4847e-02, -4.3871e-01,  6.3752e-01, -2.1017e-01, -2.9291e-01,\n",
       "          -3.1616e-01,  5.4117e-01,  1.6768e-01,  9.9424e-01, -9.4510e-02,\n",
       "          -2.9022e-01, -2.1886e-03, -1.5720e-01,  2.8317e-01, -2.9364e-01,\n",
       "          -9.9998e-01,  1.4066e-01,  9.1604e-02,  1.1458e-01, -2.1965e-01,\n",
       "           3.0746e-01, -5.7720e-02, -8.7692e-01, -9.3892e-02,  2.2809e-01,\n",
       "           3.8768e-02, -3.2828e-01, -3.1139e-01,  4.1117e-01,  4.6004e-01,\n",
       "           5.5266e-01,  7.2535e-01,  2.5635e-01,  5.2958e-01,  4.7964e-01,\n",
       "          -1.0402e-01, -5.4204e-01,  8.4934e-01]], grad_fn=<TanhBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state, outputs.pooler_output,outputs.cross_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba448319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n",
      "torch.Size([1, 768])\n",
      "cross_attentions attribute does not exist in the outputs.\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "\n",
    "# Shape of last_hidden_state\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# Shape of pooler_output\n",
    "print(outputs.pooler_output.shape)\n",
    "\n",
    "# Check if cross_attentions exist\n",
    "if outputs.cross_attentions is not None:\n",
    "    # Shapes of cross_attentions\n",
    "    for i, attentions in enumerate(outputs.cross_attentions):\n",
    "        print(f\"Shape of cross_attentions[{i}]: {attentions.shape}\")\n",
    "else:\n",
    "    print(\"cross_attentions attribute does not exist in the outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1308e4e7",
   "metadata": {},
   "source": [
    "The `last_hidden_state` tensor has three dimensions: `(batch_size, sequence_length, hidden_size)`. Here's what each dimension represents:\n",
    "\n",
    "1. `batch_size`: The number of input sequences processed together in a batch. It indicates how many sequences are processed simultaneously during inference or training.\n",
    "\n",
    "2. `sequence_length`: The length of the longest input sequence in the batch. It represents the number of tokens in each sequence.\n",
    "\n",
    "3. `hidden_size`: The size of the hidden state representation for each token. It indicates the dimensionality of the contextualized representation for each token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2ee75fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ -7.8962,  -7.8105,  -7.7903,  ...,  -7.0694,  -7.1693,  -4.3590],\n",
       "          [ -8.4461,  -8.4401,  -8.5044,  ...,  -8.0625,  -7.9909,  -5.7160],\n",
       "          [-15.2953, -15.4727, -15.5865,  ..., -12.9857, -11.7038, -11.4293],\n",
       "          ...,\n",
       "          [-14.0628, -14.2535, -14.3645,  ..., -12.7151, -11.1621, -10.2317],\n",
       "          [-10.6576, -10.7892, -11.0402,  ..., -10.3233, -10.1578,  -3.7722],\n",
       "          [-11.3383, -11.4590, -11.1767,  ...,  -9.2152,  -9.5209,  -9.5571]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " tensor([[ 3.3474, -2.0613]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "prediction_scores, seq_relationship_scores = outputs[:2]\n",
    "prediction_scores, seq_relationship_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f44d82",
   "metadata": {},
   "source": [
    "## Text/Sequence Classification using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a21284d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class:  Positive\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the input text\n",
    "text = \"I really enjoyed watching this movie. The acting and plot were excellent!\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Perform text classification\n",
    "outputs = model(**inputs)\n",
    "# logits are raw output scores for each class.\n",
    "logits = outputs.logits\n",
    "\n",
    "# Interpretation of logits\n",
    "# To interpret the logits, we use the `argmax` function to select the class with the highest score (`predicted_class`). \n",
    "#We also create a list of class names, where the index corresponds to the class label.\n",
    "predicted_class = logits.argmax(dim=1)\n",
    "class_names = ['Negative', 'Positive']\n",
    "predicted_label = class_names[predicted_class.item()]\n",
    "\n",
    "#  Finally, we retrieve the predicted label by mapping the predicted class to its corresponding class name using \n",
    "# `class_names[predicted_class.item()]`.\n",
    "# Print the predicted class label\n",
    "print(\"Predicted class: \", predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a11ea",
   "metadata": {},
   "source": [
    "### Named Entity Recognition using high-level API pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bf5aba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': 0.9990139, 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': 0.999645, 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35e79f",
   "metadata": {},
   "source": [
    "## Question Answering Task: has an API pipeline from the model distilbert by default "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4336aebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9538117051124573, 'start': 31, 'end': 39, 'answer': 'İstanbul'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_model = pipeline(\"question-answering\")\n",
    "question = \"Where do I live?\"\n",
    "context = \"My name is Merve and I live in İstanbul.\"\n",
    "qa_model(question = question, context = context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4ac1a",
   "metadata": {},
   "source": [
    "## Sentiment Analysis (API pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78ee7306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.4749954640865326}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "t = \"I like you\"\n",
    "pipe(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fce47a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998656511306763},\n",
       " {'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3da42",
   "metadata": {},
   "source": [
    "## <center> XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014e719",
   "metadata": {},
   "source": [
    "https://huggingface.co/xlm-roberta-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf375dd",
   "metadata": {},
   "source": [
    "XLM-RoBERTa is a multilingual version of RoBERTa (another version that uses BERT). It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages.It can predict masked tokens and classify text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbd6fea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10563922673463821,\n",
       "  'token': 54543,\n",
       "  'token_str': 'fashion',\n",
       "  'sequence': \"Hello I'm a fashion model.\"},\n",
       " {'score': 0.08015299588441849,\n",
       "  'token': 3525,\n",
       "  'token_str': 'new',\n",
       "  'sequence': \"Hello I'm a new model.\"},\n",
       " {'score': 0.03341350704431534,\n",
       "  'token': 3299,\n",
       "  'token_str': 'model',\n",
       "  'sequence': \"Hello I'm a model model.\"},\n",
       " {'score': 0.03021792322397232,\n",
       "  'token': 92265,\n",
       "  'token_str': 'French',\n",
       "  'sequence': \"Hello I'm a French model.\"},\n",
       " {'score': 0.02643618918955326,\n",
       "  'token': 17473,\n",
       "  'token_str': 'sexy',\n",
       "  'sequence': \"Hello I'm a sexy model.\"}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='xlm-roberta-base')\n",
    "unmasker(\"Hello I'm a <mask> model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdb42004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 6.4861e+01,  1.6882e-02,  3.7656e+01,  ...,  2.1584e+01,\n",
       "           1.4380e+01,  1.8790e+01],\n",
       "         [ 2.7493e+01, -1.4091e+00,  6.4847e+01,  ...,  4.0234e+01,\n",
       "           1.6296e+01,  3.0925e+01],\n",
       "         [ 1.9604e+01, -1.2597e+00,  4.8981e+01,  ...,  3.5830e+01,\n",
       "           1.7145e+01,  2.7173e+01],\n",
       "         ...,\n",
       "         [ 2.2920e+01, -1.4657e+00,  5.1211e+01,  ...,  3.8495e+01,\n",
       "           1.6508e+01,  2.7687e+01],\n",
       "         [ 2.8598e+01, -1.2868e+00,  6.7706e+01,  ...,  4.4857e+01,\n",
       "           1.8004e+01,  3.5004e+01],\n",
       "         [ 4.4955e+01, -2.1554e-01,  4.9643e+01,  ...,  2.8253e+01,\n",
       "           1.6841e+01,  2.3610e+01]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "# prepare input\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# forward pass\n",
    "output = model(**encoded_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e9be09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"xlm-roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.30.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import XLMRobertaConfig, XLMRobertaModel\n",
    "\n",
    "# Initializing a XLM-RoBERTa xlm-roberta-base style configuration\n",
    "configuration = XLMRobertaConfig()\n",
    "\n",
    "# Initializing a model (with random weights) from the xlm-roberta-base style configuration\n",
    "model = XLMRobertaModel(configuration)\n",
    "\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f499a6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class:  Negative\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, XLMRobertaForCausalLM, AutoConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "config = AutoConfig.from_pretrained(\"roberta-base\")\n",
    "config.is_decoder = True\n",
    "model = XLMRobertaForCausalLM.from_pretrained(\"roberta-base\", config=config)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits[0]\n",
    "\n",
    "predicted_class = logits.argmax(dim=1)\n",
    "class_names = ['Negative', 'Positive']\n",
    "predicted_label = class_names[predicted_class.tolist()[0]]\n",
    "\n",
    "# Finally, we retrieve the predicted label by mapping the predicted class to its corresponding class name using \n",
    "# `class_names[predicted_class.tolist()[0]]`.\n",
    "# Print the predicted class label\n",
    "print(\"Predicted class: \", predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d5d98",
   "metadata": {},
   "source": [
    "## <center> T5 Model (Text-to-Text Transformer Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc140a9a",
   "metadata": {},
   "source": [
    "T5 model was developed by Google Research and currently supports English, French, German, and Romanian. It has 220 M parameters. https://huggingface.co/transformers/v3.0.2/notebooks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ab74fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Haus ist wunderbar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deepak\\miniconda3\\envs\\NNExperiments\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration \n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer('translate English to German: The house is wonderful.', return_tensors='pt').input_ids\n",
    "#he `.input_ids` method is then used to extract the input_ids from the dictionary-like object. \n",
    "#The input_ids represent the numerical representation of the tokenized input text that can be understood by the T5 model.\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# Das Haus ist wunderbar.,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43564a9",
   "metadata": {},
   "source": [
    "### Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5f25f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deepak\\miniconda3\\envs\\NNExperiments\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 100, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the T5 model can be used for various natural language processing tasks . it can generate high-quality summaries of long documents or articles .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
    "text = \"The T5 model is a versatile language model that can be used for various natural language processing tasks. It has been trained on a large amount of text data and can generate high-quality summaries of long documents or articles.\"\n",
    "summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0785d",
   "metadata": {},
   "source": [
    "### Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5be17826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo, wie sind Sie?\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_de\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
    "english_text = \"Hello, how are you?\"\n",
    "german_translation = translator(english_text)\n",
    "print(german_translation[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44256783",
   "metadata": {},
   "source": [
    "### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d35ab29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a language model\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answering = pipeline(\"question-answering\")\n",
    "context = \"T5 is a language model developed by the Hugging Face team. It can be used for various natural language processing tasks.\"\n",
    "question = \"What is T5?\"\n",
    "answer = question_answering(question=question, context=context)\n",
    "print(answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2740195",
   "metadata": {},
   "source": [
    "### Text Completion or Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b36d0873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to the  I am going to the I am going to the I am going to the I am going to the I am going to the I am going to the I am going to the I am going to the I am going to the I am going to the I am going to I am going to I am going to I am going to I am going to I am going to I am going to I am going to I am going to I am going to I am going to\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_completion = pipeline(\"text-generation\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
    "text = \"I am going to the \"\n",
    "completed_text = text_completion(text, max_length=100, do_sample=False)\n",
    "print(completed_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b8a0cc",
   "metadata": {},
   "source": [
    "### Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6092c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going for a walk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "paraphraser = pipeline(\"text2text-generation\", model=\"t5-base\", tokenizer=\"t5-base\")\n",
    "text = \"I am going for a walk.\"\n",
    "paraphrase = paraphraser(text, max_length=100, do_sample=False)\n",
    "print(paraphrase[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b813124",
   "metadata": {},
   "source": [
    "### Comparisions between BERT and T5: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16434a64",
   "metadata": {},
   "source": [
    "1. BERT (Bidirectional Encoder Representations from Transformers):\n",
    "   - BERT-base: It has around 110 million parameters.\n",
    "   - BERT-large: It has around 340 million parameters.\n",
    "\n",
    "BERT is has a large number of parameters, allowing it to capture intricate patterns and representations in the data. The large size of BERT enables it to learn and generate high-quality contextualized word embeddings, which can be beneficial for various NLP tasks.\n",
    "\n",
    "2. T5 (Text-to-Text Transfer Transformer):\n",
    "   - T5-base: It has around 220 million parameters.\n",
    "   - T5-large: It has around 770 million parameters.\n",
    "   - T5-3B: It has around 3 billion parameters.\n",
    "   - T5-11B: It has around 11 billion parameters.\n",
    "\n",
    "T5 is considered an LLM due to its massive number of parameters. The large size of T5 allows it to capture complex relationships between text inputs and outputs, making it highly effective for a wide range of NLP tasks.\n",
    "\n",
    "BERT, when compared with GPT-3, T5 is not considered as an LLM because it does not have Billions of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe5b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNExperiments",
   "language": "python",
   "name": "nnexperiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
